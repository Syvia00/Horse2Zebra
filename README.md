# Horse2Zebra with AttentionGan

## Introduction
The aim of this project is to explore image to image translation using advanced General Adversarial network Techniques, specifically through horse to zebra transformations. The challenge we face is to perform efficient image translation with only an unpaired data set and get high quality realistic results. We are aiming to discover the effects of whether the more robust and simple method Cycle GAN can compare to the results of Attention Gan, a model that is of higher complexity but less efficient. For their applications, these models can be used to generate stylized images for creative applications, such as transforming images into art or other domain-specific styles. We fine tuned several parameters and applied normalisation to images to improve model behaviours.

## Method
We preprocessed images by normalising, cropping and flipping to increase the diversity of the dataset. For the main model to transfer horses to zebras, we use AttentionGAN due to their ability to perform well on unpaired style transfer tasks.
We modified the learning rate for GANs to allow the model to better balance the generator and discriminator, enabling quicker convergence without significant gradient oscillation. We also adjusted several other parameters like crop size, batch size, ngf and ndf to optimise the training process for both models

## Experimental Setup
The dataset contained 2551 images of horses and zebras; they are in the jpg format and are all coloured images. The dataset is balanced with 55% of the images being horses and 45% being zebras. The training and testing datasets have an even distribution that is the same as the whole database with the training set being 90% of the images and testing data being 10% of the images.  The images contain a wide variety of types of shots. The horses and zebras have single animal closeups, multi-animal closeups, front on and side on images as well as images with the subject in the distance. 
The images came from the kaggle dataset :
https://www.kaggle.com/datasets/balraj98/horse2zebra-dataset 

We have to convert all images to 256x256 pixels to run through our network, normalise the images by converting them to tensor format and scale values to the [-1,1] range, leading to more stable training and faster convergence. We also employed grayscale conversion to streamline image complexity from three channels to one, reducing memory requirements and accelerating the training process. 
Evaluating the quality of our results was a challenging task as to truly get accurate readings of how realistic image transfers are we need humans to rate the images. The papers on the models solved this issue firstly by hiring a large group of people to go through and rate the accuracy of their images then summed the scores. Secondly they used metrics such as frechet inception distance (FID). Fid is a commonly used metric for GANâ€™s trained on large datasets of real images to give a score based on how realistic an image is compared to its training set. The range is from 0 to infinity where 0 is the most realistic. 
We were able to evaluate our images using the fid functions from torch, however to make a metric comparable to the other papers human scores we utilised an ai animal detection software. With this software we got it to analyse the image and see if it could accurately detect the animal it was meant to be. Then we would analyse the confidence scores of the correct animal. We took the mean confidence scores of all the testing images, as well as measuring wether or not the Ai was able to correctly identify the animal in the image. 
There are some hyperparameters set to optimise the training process of the model. The NGF and NDF (number of feature maps for the generator and discriminator) are set to 64 balancing model capacity and computational cost. The batch size is set to 4, which is smaller than the default to suit limited computational resources.
