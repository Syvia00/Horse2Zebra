{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9444 HorseToZebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model and Inital Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import dominate\n",
    "from dominate.tags import meta, h3, table, tr, td, p, a, img, br\n",
    "import functools\n",
    "import itertools\n",
    "import importlib\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import multiprocessing\n",
    "from multiprocessing import freeze_support\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ntpath\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "import random\n",
    "import sys\n",
    "from subprocess import Popen, PIPE\n",
    "from scipy.stats import gaussian_kde\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "opt = {\n",
    "    # Basic parameters\n",
    "    'dataroot': './datasets',\n",
    "    'name': 'horse2zebra_attentiongan',\n",
    "    'gpu_ids': [0],\n",
    "    'checkpoints_dir': './checkpoints',\n",
    "    \n",
    "    # Model parameters\n",
    "    'model' : 'attention_gan',\n",
    "    'input_nc': 3,\n",
    "    'output_nc': 3,\n",
    "    'ngf': 64,\n",
    "    'ndf': 64,\n",
    "    'netD': 'basic',\n",
    "    'netG': 'resnet_9blocks',\n",
    "    'n_layers_D': 3,\n",
    "    'norm': 'instance',\n",
    "    'init_type': 'normal',\n",
    "    'init_gain': 0.02,\n",
    "    'no_dropout': True,\n",
    "    \n",
    "    # Dataset parameters\n",
    "    'dataset_mode': 'unaligned',\n",
    "    'direction': 'AtoB',\n",
    "    'serial_batches': False,\n",
    "    'num_threads': 4,\n",
    "    'batch_size': 4,\n",
    "    'load_size': 286,\n",
    "    'crop_size': 256,\n",
    "    'max_dataset_size': float(\"inf\"),\n",
    "    'preprocess': 'resize_and_crop',\n",
    "    'no_flip': False,\n",
    "    'display_winsize': 256,\n",
    "    \n",
    "    # Training parameters\n",
    "    'phase': 'train',\n",
    "    'niter': 60,\n",
    "    'niter_decay': 0,\n",
    "    'beta1': 0.5,\n",
    "    'lr': 0.0002,\n",
    "    'gan_mode': 'lsgan',\n",
    "    'pool_size': 50,\n",
    "    'lambda_A': 10.0,\n",
    "    'lambda_B': 10.0,\n",
    "    'lambda_identity': 0.5,\n",
    "    'isTrain': True,\n",
    "    'lr_policy': 'linear',\n",
    "    \n",
    "    # Display parameters\n",
    "    'display_freq': 100,\n",
    "    'display_id':1,\n",
    "    'display_ncols':10,\n",
    "    'no_html':True,\n",
    "    'display_port':8097,\n",
    "    'display_server':\"http://localhost\",\n",
    "    'display_env':'main',\n",
    "    'print_freq': 100,\n",
    "    'save_latest_freq': 5000,\n",
    "    'save_epoch_freq': 5,\n",
    "    'save_by_iter': False,\n",
    "    'continue_train': False,\n",
    "    'epoch_count': 1,\n",
    "    'update_html_freq':1000,\n",
    "    'save_result': False,\n",
    "\n",
    "    # additional\n",
    "    'saveDisk':True,\n",
    "    'verbose':True,\n",
    "}\n",
    "\n",
    "# Image handling utilities\n",
    "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n",
    "\n",
    "freeze_support()\n",
    "\n",
    "# Create output directories\n",
    "output_dir = os.path.join(opt['checkpoints_dir'], opt['name'])\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used Function for DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(data.Dataset, ABC):\n",
    "    def __init__(self):\n",
    "        self.opt = opt\n",
    "        self.root = opt['dataroot']\n",
    "\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train):\n",
    "        return parser\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "\n",
    "def get_params(opt, size):\n",
    "    w, h = size\n",
    "    new_h = h\n",
    "    new_w = w\n",
    "    if opt['preprocess'] == 'resize_and_crop':\n",
    "        new_h = new_w = opt['load_size']\n",
    "    elif opt['preprocess'] == 'scale_width_and_crop':\n",
    "        new_w = opt['load_size']\n",
    "        new_h = opt['load_size'] * h // w\n",
    "\n",
    "    x = random.randint(0, np.maximum(0, new_w - opt['crop_size']))\n",
    "    y = random.randint(0, np.maximum(0, new_h - opt['crop_size']))\n",
    "\n",
    "    flip = random.random() > 0.5\n",
    "\n",
    "    return {'crop_pos': (x, y), 'flip': flip}\n",
    "    \n",
    "def get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, convert=True):\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "    if 'resize' in opt['preprocess']:\n",
    "        osize = [opt['load_size'], opt['load_size']]\n",
    "        transform_list.append(transforms.Resize(osize, method))\n",
    "    elif 'scale_width' in opt['preprocess']:\n",
    "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt['load_size'], method)))\n",
    "\n",
    "    if 'crop' in opt['preprocess']:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomCrop(opt['crop_size']))\n",
    "        else:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt['crop_size'])))\n",
    "\n",
    "    if opt['preprocess'] == 'none':\n",
    "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n",
    "\n",
    "    if not opt['no_flip']:\n",
    "        if params is None:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        elif params['flip']:\n",
    "            transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if grayscale:\n",
    "            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n",
    "        else:\n",
    "            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "def __make_power_2(img, base, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    h = int(round(oh / base) * base)\n",
    "    w = int(round(ow / base) * base)\n",
    "    if (h == oh) and (w == ow):\n",
    "        return img\n",
    "\n",
    "    __print_size_warning(ow, oh, w, h)\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
    "    ow, oh = img.size\n",
    "    if (ow == target_width):\n",
    "        return img\n",
    "    w = target_width\n",
    "    h = int(target_width * oh / ow)\n",
    "    return img.resize((w, h), method)\n",
    "\n",
    "def __crop(img, pos, size):\n",
    "    ow, oh = img.size\n",
    "    x1, y1 = pos\n",
    "    tw = th = size\n",
    "    if (ow > tw or oh > th):\n",
    "        return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "    return img\n",
    "\n",
    "def __flip(img, flip):\n",
    "    if flip:\n",
    "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return img\n",
    "\n",
    "def __print_size_warning(ow, oh, w, h):\n",
    "    if not hasattr(__print_size_warning, 'has_printed'):\n",
    "        print(\"The image size needs to be a multiple of 4. \"\n",
    "              \"The loaded image size was (%d, %d), so it was adjusted to \"\n",
    "              \"(%d, %d). This adjustment will be done to all images \"\n",
    "              \"whose sizes are not multiples of 4\" % (ow, oh, w, h))\n",
    "        __print_size_warning.has_printed = True\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def make_dataset(dir, max_dataset_size=float(\"inf\")):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images[:min(max_dataset_size, len(images))]\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class ImageFolder(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, transform=None, return_paths=False,\n",
    "                 loader=default_loader):\n",
    "        imgs = make_dataset(root)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" +\n",
    "                               \",\".join(IMG_EXTENSIONS)))\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.return_paths = return_paths\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.return_paths:\n",
    "            return img, path\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "class UnalignedDataset(BaseDataset):\n",
    "    def __init__(self):\n",
    "        BaseDataset.__init__(self)\n",
    "        self.dir_A = os.path.join(opt['dataroot'], opt['phase'] + 'A')  # create a path '/path/to/data/trainA'\n",
    "        self.dir_B = os.path.join(opt['dataroot'], opt['phase'] + 'B')  # create a path '/path/to/data/trainB'\n",
    "\n",
    "        self.A_paths = sorted(make_dataset(self.dir_A, opt['max_dataset_size']))   # load images from '/path/to/data/trainA'\n",
    "        self.B_paths = sorted(make_dataset(self.dir_B, opt['max_dataset_size']))    # load images from '/path/to/data/trainB'\n",
    "        self.A_size = len(self.A_paths)  # get the size of dataset A\n",
    "        self.B_size = len(self.B_paths)  # get the size of dataset B\n",
    "        btoA = self.opt['direction'] == 'BtoA'\n",
    "        input_nc = self.opt['output_nc'] if btoA else self.opt['input_nc']       # get the number of channels of input image\n",
    "        output_nc = self.opt['input_nc'] if btoA else self.opt['output_nc']      # get the number of channels of output image\n",
    "        self.transform_A = get_transform(self.opt, grayscale=(input_nc == 1))\n",
    "        self.transform_B = get_transform(self.opt, grayscale=(output_nc == 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        A_path = self.A_paths[index % self.A_size]  # make sure index is within then range\n",
    "        if self.opt['serial_batches']:   # make sure index is within then range\n",
    "            index_B = index % self.B_size\n",
    "        else:   # randomize the index for domain B to avoid fixed pairs.\n",
    "            index_B = random.randint(0, self.B_size - 1)\n",
    "        B_path = self.B_paths[index_B]\n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "        # apply image transformation\n",
    "        A = self.transform_A(A_img)\n",
    "        B = self.transform_B(B_img)\n",
    "\n",
    "        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)\n",
    "\n",
    "class CustomDatasetDataLoader():\n",
    "    def __init__(self):\n",
    "        dataset_class = UnalignedDataset()\n",
    "        self.dataset = dataset_class\n",
    "        print(\"dataset [%s] was created\" % type(self.dataset).__name__)\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=opt['batch_size'],\n",
    "            shuffle=not opt['serial_batches'],\n",
    "            num_workers=0)\n",
    "\n",
    "    def load_data(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), opt['max_dataset_size'])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            if i * opt['batch_size'] >= opt['max_dataset_size']:\n",
    "                break\n",
    "            yield data\n",
    "\n",
    "def create_dataset():\n",
    "    data_loader = CustomDatasetDataLoader()\n",
    "    dataset = data_loader.load_data()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 1334\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset and model\n",
    "dataset = create_dataset()\n",
    "dataset_size = len(dataset)\n",
    "print('The number of training images = %d' % dataset_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Function for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = lambda x: Identity()\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "    if opt['lr_policy'] == 'linear':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch + opt['epoch_count'] - opt['niter']) / float(opt['niter_decay'] + 1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt['lr_policy'] == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt['lr_decay_iters'], gamma=0.1)\n",
    "    elif opt['lr_policy'] == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    elif opt['lr_policy'] == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt['niter'], eta_min=0)\n",
    "    else:\n",
    "        return NotImplementedError('learning rate policy [%s] is not implemented', opt['lr_policy'])\n",
    "    return scheduler\n",
    "\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n",
    "\n",
    "\n",
    "def init_net(net, init_type='normal', init_gain=0.02):\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net\n",
    "\n",
    "\n",
    "def define_G(input_nc, output_nc, ngf, netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netG == 'resnet_9blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n",
    "    elif netG == 'unet_128':\n",
    "        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'unet_256':\n",
    "        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'our':\n",
    "        net = ResnetGenerator_our(input_nc, output_nc, ngf, n_blocks=9)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)\n",
    "    return init_net(net, init_type, init_gain)\n",
    "\n",
    "\n",
    "def define_D(input_nc, ndf, netD, n_layers_D=3, norm='batch', init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netD == 'basic':  # default PatchGAN classifier\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n",
    "    elif netD == 'n_layers':  # more options\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n",
    "    elif netD == 'pixel':     # classify if each pixel is real or fake\n",
    "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n",
    "    else:\n",
    "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % netD)\n",
    "    return init_net(net, init_type, init_gain)\n",
    "\n",
    "\n",
    "################################################\n",
    "#                  Classes                     #\n",
    "################################################\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wgangp']:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def cal_gradient_penalty(netD, real_data, fake_data, device, type='mixed', constant=1.0, lambda_gp=10.0):\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1, device=device)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n",
    "\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
    "\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "class ResnetGenerator_our(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=9):\n",
    "        super(ResnetGenerator_our, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        self.nb = n_blocks\n",
    "        self.conv1 = nn.Conv2d(input_nc, ngf, 7, 1, 0)\n",
    "        self.conv1_norm = nn.InstanceNorm2d(ngf)\n",
    "        self.conv2 = nn.Conv2d(ngf, ngf * 2, 3, 2, 1)\n",
    "        self.conv2_norm = nn.InstanceNorm2d(ngf * 2)\n",
    "        self.conv3 = nn.Conv2d(ngf * 2, ngf * 4, 3, 2, 1)\n",
    "        self.conv3_norm = nn.InstanceNorm2d(ngf * 4)\n",
    "\n",
    "        self.resnet_blocks = []\n",
    "        for i in range(n_blocks):\n",
    "            self.resnet_blocks.append(resnet_block(ngf * 4, 3, 1, 1))\n",
    "            self.resnet_blocks[i].weight_init(0, 0.02)\n",
    "\n",
    "        self.resnet_blocks = nn.Sequential(*self.resnet_blocks)\n",
    "\n",
    "        self.deconv1_content = nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, 1)\n",
    "        self.deconv1_norm_content = nn.InstanceNorm2d(ngf * 2)\n",
    "        self.deconv2_content = nn.ConvTranspose2d(ngf * 2, ngf, 3, 2, 1, 1)\n",
    "        self.deconv2_norm_content = nn.InstanceNorm2d(ngf)\n",
    "        self.deconv3_content = nn.Conv2d(ngf, 27, 7, 1, 0)\n",
    "\n",
    "        self.deconv1_attention = nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, 1)\n",
    "        self.deconv1_norm_attention = nn.InstanceNorm2d(ngf * 2)\n",
    "        self.deconv2_attention = nn.ConvTranspose2d(ngf * 2, ngf, 3, 2, 1, 1)\n",
    "        self.deconv2_norm_attention = nn.InstanceNorm2d(ngf)\n",
    "        self.deconv3_attention = nn.Conv2d(ngf, 10, 1, 1, 0)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        x = F.pad(input, (3, 3, 3, 3), 'reflect')\n",
    "        x = F.relu(self.conv1_norm(self.conv1(x)))\n",
    "        x = F.relu(self.conv2_norm(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_norm(self.conv3(x)))\n",
    "        x = self.resnet_blocks(x)\n",
    "        x_content = F.relu(self.deconv1_norm_content(self.deconv1_content(x)))\n",
    "        x_content = F.relu(self.deconv2_norm_content(self.deconv2_content(x_content)))\n",
    "        x_content = F.pad(x_content, (3, 3, 3, 3), 'reflect')\n",
    "        content = self.deconv3_content(x_content)\n",
    "        image = self.tanh(content)\n",
    "        image1 = image[:, 0:3, :, :]\n",
    "        # print(image1.size()) # [1, 3, 256, 256]\n",
    "        image2 = image[:, 3:6, :, :]\n",
    "        image3 = image[:, 6:9, :, :]\n",
    "        image4 = image[:, 9:12, :, :]\n",
    "        image5 = image[:, 12:15, :, :]\n",
    "        image6 = image[:, 15:18, :, :]\n",
    "        image7 = image[:, 18:21, :, :]\n",
    "        image8 = image[:, 21:24, :, :]\n",
    "        image9 = image[:, 24:27, :, :]\n",
    "\n",
    "        x_attention = F.relu(self.deconv1_norm_attention(self.deconv1_attention(x)))\n",
    "        x_attention = F.relu(self.deconv2_norm_attention(self.deconv2_attention(x_attention)))\n",
    "        # x_attention = F.pad(x_attention, (3, 3, 3, 3), 'reflect')\n",
    "        # print(x_attention.size()) [1, 64, 256, 256]\n",
    "        attention = self.deconv3_attention(x_attention)\n",
    "\n",
    "        softmax_ = torch.nn.Softmax(dim=1)\n",
    "        attention = softmax_(attention)\n",
    "\n",
    "        attention1_ = attention[:, 0:1, :, :]\n",
    "        attention2_ = attention[:, 1:2, :, :]\n",
    "        attention3_ = attention[:, 2:3, :, :]\n",
    "        attention4_ = attention[:, 3:4, :, :]\n",
    "        attention5_ = attention[:, 4:5, :, :]\n",
    "        attention6_ = attention[:, 5:6, :, :]\n",
    "        attention7_ = attention[:, 6:7, :, :]\n",
    "        attention8_ = attention[:, 7:8, :, :]\n",
    "        attention9_ = attention[:, 8:9, :, :]\n",
    "        attention10_ = attention[:, 9:10, :, :]\n",
    "\n",
    "        attention1 = attention1_.repeat(1, 3, 1, 1)\n",
    "        # print(attention1.size())\n",
    "        attention2 = attention2_.repeat(1, 3, 1, 1)\n",
    "        attention3 = attention3_.repeat(1, 3, 1, 1)\n",
    "        attention4 = attention4_.repeat(1, 3, 1, 1)\n",
    "        attention5 = attention5_.repeat(1, 3, 1, 1)\n",
    "        attention6 = attention6_.repeat(1, 3, 1, 1)\n",
    "        attention7 = attention7_.repeat(1, 3, 1, 1)\n",
    "        attention8 = attention8_.repeat(1, 3, 1, 1)\n",
    "        attention9 = attention9_.repeat(1, 3, 1, 1)\n",
    "        attention10 = attention10_.repeat(1, 3, 1, 1)\n",
    "\n",
    "        output1 = image1 * attention1\n",
    "        output2 = image2 * attention2\n",
    "        output3 = image3 * attention3\n",
    "        output4 = image4 * attention4\n",
    "        output5 = image5 * attention5\n",
    "        output6 = image6 * attention6\n",
    "        output7 = image7 * attention7\n",
    "        output8 = image8 * attention8\n",
    "        output9 = image9 * attention9\n",
    "        # output10 = image10 * attention10\n",
    "        output10 = input * attention10\n",
    "\n",
    "        o=output1 + output2 + output3 + output4 + output5 + output6 + output7 + output8 + output9 + output10\n",
    "\n",
    "        return o, output1, output2, output3, output4, output5, output6, output7, output8, output9, output10, attention1,attention2,attention3, attention4, attention5, attention6, attention7, attention8,attention9,attention10, image1, image2,image3,image4,image5,image6,image7,image8,image9\n",
    "\n",
    "# resnet block with reflect padding\n",
    "class resnet_block(nn.Module):\n",
    "    def __init__(self, channel, kernel, stride, padding):\n",
    "        super(resnet_block, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.kernel = kernel\n",
    "        self.strdie = stride\n",
    "        self.padding = padding\n",
    "        self.conv1 = nn.Conv2d(channel, channel, kernel, stride, 0)\n",
    "        self.conv1_norm = nn.InstanceNorm2d(channel)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel, stride, 0)\n",
    "        self.conv2_norm = nn.InstanceNorm2d(channel)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.pad(input, (self.padding, self.padding, self.padding, self.padding), 'reflect')\n",
    "        x = F.relu(self.conv1_norm(self.conv1(x)))\n",
    "        x = F.pad(x, (self.padding, self.padding, self.padding, self.padding), 'reflect')\n",
    "        x = self.conv2_norm(self.conv2(x))\n",
    "\n",
    "        return input + x\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out\n",
    "\n",
    "\n",
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class PixelDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
    "        super(PixelDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        self.net = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
    "            norm_layer(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
    "\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.net(input)\n",
    "\n",
    "class ImagePool():\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:  # create an empty pool\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "\n",
    "        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            image = torch.unsqueeze(image.data, 0)\n",
    "            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n",
    "                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:       # by another 50% chance, the buffer will return the current image\n",
    "                    return_images.append(image)\n",
    "        return_images = torch.cat(return_images, 0)   # collect all the images and return\n",
    "        return return_images\n",
    "\n",
    "\n",
    "def save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):\n",
    "    image_dir = webpage.get_image_dir()\n",
    "    short_path = ntpath.basename(image_path[0])\n",
    "    name = os.path.splitext(short_path)[0]\n",
    "\n",
    "    webpage.add_header(name)\n",
    "    ims, txts, links = [], [], []\n",
    "\n",
    "    for label, im_data in visuals.items():\n",
    "        im = tensor2im(im_data)\n",
    "        image_name = '%s_%s.png' % (name, label)\n",
    "        save_path = os.path.join(image_dir, image_name)\n",
    "        h, w, _ = im.shape\n",
    "        if aspect_ratio > 1.0:\n",
    "            im = cv2(src=im, dsize=(h, int(w * aspect_ratio)), interpolation=cv2.INTER_CUBIC)\n",
    "        if aspect_ratio < 1.0:\n",
    "            im = cv2(src=im, dsize=(int(h / aspect_ratio), w), interpolation=cv2.INTER_CUBIC)\n",
    "        save_image(im, save_path)\n",
    "\n",
    "        ims.append(image_name)\n",
    "        txts.append(label)\n",
    "        links.append(image_name)\n",
    "    webpage.add_images(ims, txts, links, width=width)\n",
    "\n",
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "def diagnose_network(net, name='network'):\n",
    "    \"\"\"Calculate and print the mean of average absolute(gradients)\n",
    "\n",
    "    Parameters:\n",
    "        net (torch network) -- Torch network\n",
    "        name (str) -- the name of the network\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    count = 0\n",
    "    for param in net.parameters():\n",
    "        if param.grad is not None:\n",
    "            mean += torch.mean(torch.abs(param.grad.data))\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        mean = mean / count\n",
    "    print(name)\n",
    "    print(mean)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    \"\"\"Save a numpy image to the disk\n",
    "\n",
    "    Parameters:\n",
    "        image_numpy (numpy array) -- input numpy array\n",
    "        image_path (str)          -- the path of the image\n",
    "    \"\"\"\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)\n",
    "\n",
    "\n",
    "def print_numpy(x, val=True, shp=False):\n",
    "    \"\"\"Print the mean, min, max, median, std, and size of a numpy array\n",
    "\n",
    "    Parameters:\n",
    "        val (bool) -- if print the values of the numpy array\n",
    "        shp (bool) -- if print the shape of the numpy array\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float64)\n",
    "    if shp:\n",
    "        print('shape,', x.shape)\n",
    "    if val:\n",
    "        x = x.flatten()\n",
    "        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n",
    "            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.device = torch.device('cpu')\n",
    "        self.isTrain = opt['isTrain']\n",
    "        self.save_dir = os.path.join(opt['checkpoints_dir'], opt['name'])\n",
    "        if opt['preprocess'] != 'scale_width':\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.loss_names = []\n",
    "        self.model_names = []\n",
    "        self.visual_names = []\n",
    "        self.optimizers = []\n",
    "        self.image_paths = []\n",
    "        self.metric = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train):\n",
    "        return parser\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_input(self, input):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def optimize_parameters(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, opt):\n",
    "        \"\"\"Initialize networks and optimizers\"\"\"\n",
    "        if self.isTrain:\n",
    "            self.schedulers = [get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
    "        if not self.isTrain or opt['continue_train']:\n",
    "            load_suffix = 'iter_%d' % opt['load_iter'] if opt['load_iter'] > 0 else opt['epoch']\n",
    "            self.load_networks(load_suffix)\n",
    "        self.print_networks(opt['verbose'])\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Make models eval mode during test time\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                net = getattr(self, 'net' + name)\n",
    "                net.eval()\n",
    "\n",
    "    def test(self):\n",
    "        with torch.no_grad():\n",
    "            self.forward()\n",
    "            self.compute_visuals()\n",
    "\n",
    "    def compute_visuals(self):\n",
    "        \"\"\"Calculate additional output images for visdom and HTML visualization\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_image_paths(self):\n",
    "        \"\"\" Return image paths that are used to load current data\"\"\"\n",
    "        return self.image_paths\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n",
    "        for scheduler in self.schedulers:\n",
    "            if self.opt['lr_policy'] == 'plateau':\n",
    "                scheduler.step(self.metric)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "\n",
    "        lr = self.optimizers[0].param_groups[0]['lr']\n",
    "        print('learning rate = %.7f' % lr)\n",
    "\n",
    "    def get_current_visuals(self):\n",
    "        \"\"\"Return visualization images. train.py will display these images with visdom, and save the images to a HTML\"\"\"\n",
    "        visual_ret = OrderedDict()\n",
    "        for name in self.visual_names:\n",
    "            if isinstance(name, str):\n",
    "                visual_ret[name] = getattr(self, name)\n",
    "        return visual_ret\n",
    "\n",
    "    def get_current_losses(self):\n",
    "        \"\"\"Return traning losses / errors. train.py will print out these errors on console, and save them to a file\"\"\"\n",
    "        errors_ret = OrderedDict()\n",
    "        for name in self.loss_names:\n",
    "            if isinstance(name, str):\n",
    "                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number\n",
    "        return errors_ret\n",
    "\n",
    "    def save_networks(self, epoch):\n",
    "        \"\"\"Save model networks\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                save_filename = '%s_net_%s.pth' % (epoch, name)\n",
    "                save_path = os.path.join(self.save_dir, save_filename)\n",
    "                net = getattr(self, 'net' + name)\n",
    "                torch.save(net.state_dict(), save_path)\n",
    "\n",
    "    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n",
    "        \"\"\"Fix InstanceNorm checkpoints incompatibility (prior to 0.4)\"\"\"\n",
    "        key = keys[i]\n",
    "        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n",
    "            if module.__class__.__name__.startswith('InstanceNorm') and \\\n",
    "                    (key == 'running_mean' or key == 'running_var'):\n",
    "                if getattr(module, key) is None:\n",
    "                    state_dict.pop('.'.join(keys))\n",
    "            if module.__class__.__name__.startswith('InstanceNorm') and \\\n",
    "               (key == 'num_batches_tracked'):\n",
    "                state_dict.pop('.'.join(keys))\n",
    "        else:\n",
    "            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n",
    "\n",
    "    def load_networks(self, epoch):\n",
    "        \"\"\"Load model networks\"\"\"\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                load_filename = '%s_net_%s.pth' % (epoch, name)\n",
    "                load_path = os.path.join(self.save_dir, load_filename)\n",
    "                net = getattr(self, 'net' + name)\n",
    "                state_dict = torch.load(load_path, map_location=self.device)\n",
    "                net.load_state_dict(state_dict)\n",
    "\n",
    "    def print_networks(self, verbose):\n",
    "\n",
    "        print('---------- Networks initialized -------------')\n",
    "        for name in self.model_names:\n",
    "            if isinstance(name, str):\n",
    "                net = getattr(self, 'net' + name)\n",
    "                num_params = 0\n",
    "                for param in net.parameters():\n",
    "                    num_params += param.numel()\n",
    "                if verbose:\n",
    "                    print(net)\n",
    "                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "class AttentionGANModel(BaseModel):\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train=True):\n",
    "        parser.set_defaults(no_dropout=True)  # default CycleGAN did not use dropout\n",
    "        if is_train:\n",
    "            parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for cycle loss (A -> B -> A)')\n",
    "            parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n",
    "            parser.add_argument('--lambda_identity', type=float, default=0.5, help='use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        BaseModel.__init__(self, opt)\n",
    "        self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n",
    "        \n",
    "        # Visual names setup\n",
    "        visual_names_A = ['real_A', 'fake_B', 'rec_A', 'o1_b', 'o2_b', 'o3_b', 'o4_b', 'o5_b', 'o6_b', 'o7_b', 'o8_b', 'o9_b', 'o10_b',\n",
    "                         'a1_b', 'a2_b', 'a3_b', 'a4_b', 'a5_b', 'a6_b', 'a7_b', 'a8_b', 'a9_b', 'a10_b', 'i1_b', 'i2_b', 'i3_b', 'i4_b', 'i5_b', \n",
    "                         'i6_b', 'i7_b', 'i8_b', 'i9_b']\n",
    "        visual_names_B = ['real_B', 'fake_A', 'rec_B', 'o1_a', 'o2_a', 'o3_a', 'o4_a', 'o5_a', 'o6_a', 'o7_a', 'o8_a', 'o9_a', 'o10_a', \n",
    "                         'a1_a', 'a2_a', 'a3_a', 'a4_a', 'a5_a', 'a6_a', 'a7_a', 'a8_a', 'a9_a', 'a10_a', 'i1_a', 'i2_a', 'i3_a', 'i4_a', 'i5_a', \n",
    "                         'i6_a', 'i7_a', 'i8_a', 'i9_a']\n",
    "\n",
    "        if self.isTrain and self.opt['lambda_identity'] > 0.0:\n",
    "            visual_names_A.append('idt_B')\n",
    "            visual_names_B.append('idt_A')\n",
    "\n",
    "        self.visual_names = visual_names_A + visual_names_B if not self.opt['saveDisk'] else ['real_A', 'fake_B', 'a10_b', 'real_B', 'fake_A', 'a10_a']\n",
    "        self.model_names = ['G_A', 'G_B', 'D_A', 'D_B'] if self.isTrain else ['G_A', 'G_B']\n",
    "\n",
    "        # Initialize networks\n",
    "        self.netG_A = define_G(opt['input_nc'], opt['output_nc'], opt['ngf'], 'our', opt['norm'],\n",
    "                              not opt['no_dropout'], opt['init_type'], opt['init_gain'])\n",
    "        self.netG_B = define_G(opt['output_nc'], opt['input_nc'], opt['ngf'], 'our', opt['norm'],\n",
    "                              not opt['no_dropout'], opt['init_type'], opt['init_gain'])\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.netD_A = define_D(opt['output_nc'], opt['ndf'], opt['netD'],\n",
    "                                 opt['n_layers_D'], opt['norm'], opt['init_type'], opt['init_gain'])\n",
    "            self.netD_B = define_D(opt['input_nc'], opt['ndf'], opt['netD'],\n",
    "                                 opt['n_layers_D'], opt['norm'], opt['init_type'], opt['init_gain'])\n",
    "\n",
    "            if opt['lambda_identity'] > 0.0:  # only works when input and output images have the same number of channels\n",
    "                assert(opt['input_nc'] == opt['output_nc'])\n",
    "            self.fake_A_pool = ImagePool(opt['pool_size'])  # create image buffer to store previously generated images\n",
    "            self.fake_B_pool = ImagePool(opt['pool_size'])  # create image buffer to store previously generated images\n",
    "            # define loss functions\n",
    "            self.criterionGAN = GANLoss(opt['gan_mode']).to(self.device)  # define GAN loss.\n",
    "            self.criterionCycle = torch.nn.L1Loss()\n",
    "            self.criterionIdt = torch.nn.L1Loss()\n",
    "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
    "            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt['lr'], betas=(opt['beta1'], 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "    def set_input(self, input):\n",
    "        AtoB = self.opt['direction'] == 'AtoB'\n",
    "        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n",
    "        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
    "        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "        self.fake_B, self.o1_b, self.o2_b, self.o3_b, self.o4_b, self.o5_b, self.o6_b, self.o7_b, self.o8_b, self.o9_b, self.o10_b, \\\n",
    "        self.a1_b, self.a2_b, self.a3_b, self.a4_b, self.a5_b, self.a6_b, self.a7_b, self.a8_b, self.a9_b, self.a10_b, \\\n",
    "        self.i1_b, self.i2_b, self.i3_b, self.i4_b, self.i5_b, self.i6_b, self.i7_b, self.i8_b, self.i9_b = self.netG_A(self.real_A)  # G_A(A)\n",
    "        self.rec_A, _, _, _, _, _, _, _, _, _, _, \\\n",
    "        _, _, _, _, _, _, _, _, _, _, \\\n",
    "        _, _, _, _, _, _, _, _, _ = self.netG_B(self.fake_B)   # G_B(G_A(A))\n",
    "        self.fake_A, self.o1_a, self.o2_a, self.o3_a, self.o4_a, self.o5_a, self.o6_a, self.o7_a, self.o8_a, self.o9_a, self.o10_a, \\\n",
    "        self.a1_a, self.a2_a, self.a3_a, self.a4_a, self.a5_a, self.a6_a, self.a7_a, self.a8_a, self.a9_a, self.a10_a, \\\n",
    "        self.i1_a, self.i2_a, self.i3_a, self.i4_a, self.i5_a, self.i6_a, self.i7_a, self.i8_a, self.i9_a = self.netG_B(self.real_B)  # G_B(B)\n",
    "        self.rec_B, _, _, _, _, _, _, _, _, _, _, \\\n",
    "        _, _, _, _, _, _, _, _, _, _, \\\n",
    "        _, _, _, _, _, _, _, _, _ = self.netG_A(self.fake_A)   # G_A(G_B(B))\n",
    "\n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\n",
    "        Parameters:\n",
    "            netD (network)      -- the discriminator D\n",
    "            real (tensor array) -- real images\n",
    "            fake (tensor array) -- images generated by a generator\n",
    "        Return the discriminator loss.\n",
    "        We also call loss_D.backward() to calculate the gradients.\n",
    "        \"\"\"\n",
    "        # Real\n",
    "        pred_real = netD(real)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "        # Fake\n",
    "        pred_fake = netD(fake.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "        # Combined loss and calculate gradients\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D.backward()\n",
    "        return loss_D\n",
    "\n",
    "    def backward_D_A(self):\n",
    "        \"\"\"Calculate GAN loss for discriminator D_A\"\"\"\n",
    "        fake_B = self.fake_B_pool.query(self.fake_B)\n",
    "        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        \"\"\"Calculate GAN loss for discriminator D_B\"\"\"\n",
    "        fake_A = self.fake_A_pool.query(self.fake_A)\n",
    "        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"Calculate the loss for generators G_A and G_B\"\"\"\n",
    "        lambda_idt = self.opt['lambda_identity']\n",
    "        lambda_A = self.opt['lambda_A']\n",
    "        lambda_B = self.opt['lambda_B']\n",
    "        # Identity loss\n",
    "        if lambda_idt > 0:\n",
    "            # G_A should be identity if real_B is fed: ||G_A(B) - B||\n",
    "            self.idt_A, _, _, _, _, _, _, _, _, _, _, \\\n",
    "            _, _, _, _, _, _, _, _, _, _, \\\n",
    "            _, _, _, _, _, _, _, _, _  = self.netG_A(self.real_B)\n",
    "            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n",
    "            # G_B should be identity if real_A is fed: ||G_B(A) - A||\n",
    "            self.idt_B, _, _, _, _, _, _, _, _, _, _, \\\n",
    "            _, _, _, _, _, _, _, _, _, _, \\\n",
    "            _, _, _, _, _, _, _, _, _  = self.netG_B(self.real_A)\n",
    "            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n",
    "        else:\n",
    "            self.loss_idt_A = 0\n",
    "            self.loss_idt_B = 0\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n",
    "        # GAN loss D_B(G_B(B))\n",
    "        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n",
    "        # Forward cycle loss || G_B(G_A(A)) - A||\n",
    "        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n",
    "        # Backward cycle loss || G_A(G_B(B)) - B||\n",
    "        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n",
    "        # combined loss and calculate gradients\n",
    "        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n",
    "        # forward\n",
    "        self.forward()      # compute fake images and reconstruction images.\n",
    "        # G_A and G_B\n",
    "        self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n",
    "        self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero\n",
    "        self.backward_G()             # calculate gradients for G_A and G_B\n",
    "        self.optimizer_G.step()       # update G_A and G_B's weights\n",
    "        # D_A and D_B\n",
    "        self.set_requires_grad([self.netD_A, self.netD_B], True)\n",
    "        self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\n",
    "        self.backward_D_A()      # calculate gradients for D_A\n",
    "        self.backward_D_B()      # calculate graidents for D_B\n",
    "        self.optimizer_D.step()  # update D_A and D_B's weights\n",
    "\n",
    "def create_model(opt):\n",
    "    instance = AttentionGANModel(opt)\n",
    "    print(\"model [%s] was created\" % type(instance).__name__)\n",
    "    return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [AttentionGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "ResnetGenerator_our(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (conv1_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (resnet_blocks): Sequential(\n",
      "    (0): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (1): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (2): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (3): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (4): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (5): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (6): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (7): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (8): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (deconv1_content): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv1_norm_content): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv2_content): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2_norm_content): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv3_content): Conv2d(64, 27, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (deconv1_attention): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv1_norm_attention): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv2_attention): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2_norm_attention): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv3_attention): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "[Network G_A] Total number of parameters : 11.823 M\n",
      "ResnetGenerator_our(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (conv1_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (resnet_blocks): Sequential(\n",
      "    (0): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (1): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (2): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (3): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (4): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (5): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (6): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (7): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "    (8): resnet_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv1_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "      (conv2_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    )\n",
      "  )\n",
      "  (deconv1_content): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv1_norm_content): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv2_content): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2_norm_content): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv3_content): Conv2d(64, 27, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (deconv1_attention): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv1_norm_attention): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv2_attention): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2_norm_attention): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (deconv3_attention): Conv2d(64, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "[Network G_B] Total number of parameters : 11.823 M\n",
      "NLayerDiscriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "[Network D_A] Total number of parameters : 2.765 M\n",
      "NLayerDiscriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "[Network D_B] Total number of parameters : 2.765 M\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = create_model(opt)      # create a model given optmodel and other options\n",
    "model.setup(opt)               # regular setup: load and print networks; create schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Graph Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_epoch_results(epoch, visuals, num_cols=4):\n",
    "    \"\"\"\n",
    "    Display epoch results using basic matplotlib\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Epoch {epoch} Visual Results ===\")\n",
    "    \n",
    "    # # Main results (real/fake images)\n",
    "    # main_visuals = ['real_A', 'fake_B', 'real_B', 'fake_A']\n",
    "    # plt.figure(figsize=(16, 4))\n",
    "    \n",
    "    # for idx, name in enumerate(main_visuals):\n",
    "    #     if name in visuals:\n",
    "    #         plt.subplot(1, 4, idx + 1)\n",
    "    #         img = visuals[name][0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    #         img = (img + 1) / 2.0  # Convert from [-1, 1] to [0, 1]\n",
    "    #         plt.imshow(img)\n",
    "    #         plt.axis('off')\n",
    "    #         plt.title(name)\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # # Attention maps visualization\n",
    "    # attention_maps = {k: v for k, v in visuals.items() if k.startswith('a') and k.endswith(('_a', '_b'))}\n",
    "    # if attention_maps:\n",
    "    #     num_att = len(attention_maps)\n",
    "    #     num_rows = (num_att + num_cols - 1) // num_cols\n",
    "    #     plt.figure(figsize=(16, 3 * num_rows))\n",
    "        \n",
    "    #     for idx, (name, att) in enumerate(attention_maps.items()):\n",
    "    #         plt.subplot(num_rows, num_cols, idx + 1)\n",
    "    #         att_map = att[0][0].cpu().detach().numpy()\n",
    "    #         im = plt.imshow(att_map, cmap='viridis')\n",
    "    #         plt.colorbar(im)\n",
    "    #         plt.title(f'Attention {name}')\n",
    "    #         plt.axis('off')\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    # # Intermediate outputs\n",
    "    # outputs = {k: v for k, v in visuals.items() if k.startswith('o') and k.endswith(('_a', '_b'))}\n",
    "    # if outputs:\n",
    "    #     num_out = len(outputs)\n",
    "    #     num_rows = (num_out + num_cols - 1) // num_cols\n",
    "    #     plt.figure(figsize=(16, 3 * num_rows))\n",
    "        \n",
    "    #     for idx, (name, out) in enumerate(outputs.items()):\n",
    "    #         plt.subplot(num_rows, num_cols, idx + 1)\n",
    "    #         img = out[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    #         img = (img + 1) / 2.0\n",
    "    #         plt.imshow(img)\n",
    "    #         plt.title(f'Output {name}')\n",
    "    #         plt.axis('off')\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    \n",
    "    if (opt['save_result']):  # save images to an HTML file if they haven't been saved.\n",
    "        opt['save_result'] = True\n",
    "        # save images to the disk\n",
    "        for label, image in visuals.items():\n",
    "            image_numpy = tensor2im(image)\n",
    "            img_path = os.path.join(opt['checkpoints_dir'], 'epoch%.3d_%s.png' % (epoch, label))\n",
    "            save_image(image_numpy, img_path)\n",
    "\n",
    "def update_and_display_losses(loss_history, epoch, current_losses, display_freq=1):\n",
    "    \"\"\"\n",
    "    Update loss history and display dynamic loss plots\n",
    "    \"\"\"\n",
    "    # Update history\n",
    "    for k, v in current_losses.items():\n",
    "        if k not in loss_history:\n",
    "            loss_history[k] = []\n",
    "        loss_history[k].append(v)\n",
    "\n",
    "    if epoch % display_freq == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Current loss values\n",
    "        print(f\"\\n=== Epoch {epoch} Loss Values ===\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Loss Type':<15} {'Current':<10} {'Mean':<10} {'Std':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for k, v in current_losses.items():\n",
    "            recent_values = loss_history[k][-10:] if len(loss_history[k]) >= 10 else loss_history[k]\n",
    "            print(f\"{k:<15} {v:10.4f} {np.mean(recent_values):10.4f} {np.std(recent_values):10.4f}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Plot loss trends\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for k, v in loss_history.items():\n",
    "            plt.plot(v, label=k)\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss Value')\n",
    "        plt.title('Training Loss Evolution')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop for Training Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/60\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb 单元格 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Training step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39mset_input(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39;49moptimize_parameters()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# # Display results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# if total_iters % opt['display_freq'] == 0:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#     model.compute_visuals()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m#     display_epoch_results(epoch, model.get_current_visuals())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Update and display losses\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m total_iters \u001b[39m%\u001b[39m opt[\u001b[39m'\u001b[39m\u001b[39mprint_freq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb 单元格 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_requires_grad([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetD_A, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetD_B], \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_D\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# set D_A and D_B's gradients to zero\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=296'>297</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward_D_A()      \u001b[39m# calculate gradients for D_A\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=297'>298</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_D_B()      \u001b[39m# calculate graidents for D_B\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=298'>299</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_D\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb 单元格 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=242'>243</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calculate GAN loss for discriminator D_A\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m fake_B \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_B_pool\u001b[39m.\u001b[39mquery(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfake_B)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/syvia/Documents/uni/UNSW/24T3/COMP9444/project/horseZebra.ipynb#X21sZmlsZQ%3D%3D?line=244'>245</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_D_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_D_basic(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetD_A, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreal_B, fake_B)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize loss history\n",
    "loss_history = {}\n",
    "total_iters = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training epochs\n",
    "for epoch in range(opt['epoch_count'], opt['niter'] + opt['niter_decay'] + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    iter_data_time = time.time()\n",
    "    epoch_iter = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{opt['niter'] + opt['niter_decay']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        iter_start_time = time.time()\n",
    "        \n",
    "        if total_iters % opt['print_freq'] == 0:\n",
    "            t_data = iter_start_time - iter_data_time\n",
    "            \n",
    "        total_iters += opt['batch_size']\n",
    "        epoch_iter += opt['batch_size']\n",
    "        \n",
    "        # Training step\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters()\n",
    "\n",
    "        # Display results\n",
    "        if total_iters % opt['display_freq'] == 0:\n",
    "            model.compute_visuals()\n",
    "            display_epoch_results(epoch, model.get_current_visuals())\n",
    "\n",
    "        # Update and display losses\n",
    "        if total_iters % opt['print_freq'] == 0:\n",
    "            losses = model.get_current_losses()\n",
    "            t_comp = (time.time() - iter_start_time) / opt['batch_size']\n",
    "            update_and_display_losses(loss_history, epoch, losses)\n",
    "\n",
    "        if total_iters % opt['save_latest_freq'] == 0:   # cache our latest model every <save_latest_freq> iterations\n",
    "            print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n",
    "            save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'\n",
    "            model.save_networks(save_suffix)\n",
    "\n",
    "        iter_data_time = time.time()\n",
    "\n",
    "    if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n",
    "            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n",
    "            model.save_networks('latest')\n",
    "            model.save_networks(epoch)\n",
    "            \n",
    "    # Print epoch timing\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print(f'\\nEpoch {epoch} completed in {time_taken:.2f} seconds')\n",
    "    print(f\"Learning rate: {model.optimizers[0].param_groups[0]['lr']:.7f}\")\n",
    "    \n",
    "    # Update learning rates\n",
    "    model.update_learning_rate()\n",
    "\n",
    "# End of training - display final analysis\n",
    "print(\"\\nTraining completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "\n",
    "# Convert loss history to numpy arrays for easier analysis\n",
    "loss_arrays = {k: np.array(v) for k, v in loss_history.items()}\n",
    "\n",
    "print(\"\\n=== Final Training Analysis ===\\n\")\n",
    "\n",
    "# 1. Statistical Summary\n",
    "print(\"Loss Statistics:\")\n",
    "print(\"-\" * 100)\n",
    "headers = ['Loss Type', 'Mean', 'Std Dev', 'Min', 'Max', 'Final', 'Initial', 'Improvement (%)']\n",
    "print(f\"{headers[0]:<15} {headers[1]:>10} {headers[2]:>10} {headers[3]:>10} {headers[4]:>10} {headers[5]:>10} {headers[6]:>10} {headers[7]:>15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for loss_type, values in loss_arrays.items():\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    final = values[-1]\n",
    "    initial = values[0]\n",
    "    improvement = ((initial - final) / initial * 100) if initial != 0 else 0\n",
    "    \n",
    "    print(f\"{loss_type:<15} {mean:10.4f} {std:10.4f} {min_val:10.4f} {max_val:10.4f} \"\n",
    "            f\"{final:10.4f} {initial:10.4f} {improvement:15.2f}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "gs = fig.add_gridspec(3, 2)\n",
    "\n",
    "# 2. Loss Evolution Plots\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "for loss_type, values in loss_arrays.items():\n",
    "    ax1.plot(values, label=loss_type, alpha=0.8, linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss Value')\n",
    "ax1.set_title('Loss Evolution Over Training', pad=20)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True)\n",
    "\n",
    "# 3. Log-scale Evolution Plot\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "for loss_type, values in loss_arrays.items():\n",
    "    ax2.semilogy(values, label=loss_type, alpha=0.8, linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss Value (log scale)')\n",
    "ax2.set_title('Loss Evolution (Log Scale)')\n",
    "ax2.grid(True)\n",
    "\n",
    "# 4. Loss Distribution Plot\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(loss_arrays)))\n",
    "for (loss_type, values), color in zip(loss_arrays.items(), colors):\n",
    "    ax3.hist(values, bins=50, alpha=0.5, label=loss_type, color=color, density=True)\n",
    "    \n",
    "    # Add KDE plot\n",
    "    kde = gaussian_kde(values)\n",
    "    x_range = np.linspace(min(values), max(values), 200)\n",
    "    ax3.plot(x_range, kde(x_range), color=color, linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Loss Value')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Loss Value Distributions')\n",
    "ax3.legend()\n",
    "\n",
    "# 5. Correlation Matrix Heatmap\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "loss_types = list(loss_arrays.keys())\n",
    "corr_matrix = np.zeros((len(loss_types), len(loss_types)))\n",
    "\n",
    "for i, loss1 in enumerate(loss_types):\n",
    "    for j, loss2 in enumerate(loss_types):\n",
    "        corr = np.corrcoef(loss_arrays[loss1], loss_arrays[loss2])[0,1]\n",
    "        corr_matrix[i,j] = corr\n",
    "\n",
    "im = ax4.imshow(corr_matrix, cmap='RdYlBu', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, ax=ax4)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(loss_types)):\n",
    "    for j in range(len(loss_types)):\n",
    "        text = ax4.text(j, i, f'{corr_matrix[i, j]:.2f}',\n",
    "                        ha='center', va='center',\n",
    "                        color='black' if abs(corr_matrix[i, j]) < 0.5 else 'white')\n",
    "\n",
    "ax4.set_xticks(range(len(loss_types)))\n",
    "ax4.set_yticks(range(len(loss_types)))\n",
    "ax4.set_xticklabels(loss_types, rotation=45)\n",
    "ax4.set_yticklabels(loss_types)\n",
    "ax4.set_title('Loss Correlation Matrix')\n",
    "\n",
    "# 6. Box Plot\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "box_data = [values for values in loss_arrays.values()]\n",
    "ax5.boxplot(box_data, labels=loss_types)\n",
    "ax5.set_xticklabels(loss_types, rotation=45)\n",
    "ax5.set_ylabel('Loss Value')\n",
    "ax5.set_title('Loss Distribution Box Plots')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Print Training Duration Analysis\n",
    "print(\"\\nTraining Progress Summary:\")\n",
    "print(\"-\" * 50)\n",
    "epochs = len(next(iter(loss_arrays.values())))\n",
    "print(f\"Total Epochs: {epochs}\")\n",
    "\n",
    "for loss_type, values in loss_arrays.items():\n",
    "    print(f\"\\n{loss_type}:\")\n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmin(values)\n",
    "    best_value = values[best_epoch]\n",
    "    \n",
    "    # Calculate convergence (when loss stabilizes within 5% of final value)\n",
    "    convergence_threshold = values[-1] * 1.05\n",
    "    convergence_epoch = np.where(values <= convergence_threshold)[0][0]\n",
    "    \n",
    "    print(f\"  Best Value: {best_value:.4f} (Epoch {best_epoch + 1})\")\n",
    "    print(f\"  Convergence Epoch: {convergence_epoch + 1}\")\n",
    "    print(f\"  Early Improvement (first 10%): {((values[0] - values[epochs//10]) / values[0] * 100):.1f}%\")\n",
    "    print(f\"  Late Improvement (last 10%): {((values[-epochs//10] - values[-1]) / values[-epochs//10] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test visual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_test_results(visuals, img_path, index):\n",
    "    \"\"\"\n",
    "    Display test results in notebook console\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Test Result {index + 1} ===\")\n",
    "    print(f\"Image Path: {img_path}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Print image statistics for each visual\n",
    "    for name, img_tensor in visuals.items():\n",
    "        # Convert tensor to numpy array\n",
    "        img = img_tensor[0].cpu().detach().numpy()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_val = np.mean(img)\n",
    "        std_val = np.std(img)\n",
    "        min_val = np.min(img)\n",
    "        max_val = np.max(img)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean: {mean_val:.4f}\")\n",
    "        print(f\"  Std Dev: {std_val:.4f}\")\n",
    "        print(f\"  Range: [{min_val:.4f}, {max_val:.4f}]\")\n",
    "        \n",
    "        # Print transformation summary\n",
    "        if name.startswith('fake'):\n",
    "            print(\"  Transformation Analysis:\")\n",
    "            reference_img = visuals[f'real_{name[-1]}'][0].cpu().detach().numpy()\n",
    "            diff = img - reference_img\n",
    "            mean_diff = np.mean(np.abs(diff))\n",
    "            print(f\"  Mean Absolute Change: {mean_diff:.4f}\")\n",
    "            print(f\"  Max Absolute Change: {np.max(np.abs(diff)):.4f}\")\n",
    "    \n",
    "    print(\"\\nAttention Analysis:\")\n",
    "    attention_maps = {k: v for k, v in visuals.items() if k.startswith('a') and k.endswith(('_a', '_b'))}\n",
    "    if attention_maps:\n",
    "        for name, att in attention_maps.items():\n",
    "            att_map = att[0][0].cpu().detach().numpy()\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Mean Attention: {np.mean(att_map):.4f}\")\n",
    "            print(f\"  Max Attention: {np.max(att_map):.4f}\")\n",
    "            print(f\"  Active Regions: {np.mean(att_map > 0.5):.1%} of image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test configuration\n",
    "opt['num_threads'] = 0\n",
    "opt['batch_size'] = 1\n",
    "opt['serial_batches'] = True\n",
    "opt['no_flip'] = True\n",
    "opt['display_id'] = -1\n",
    "\n",
    "# Create dataset and model\n",
    "print(\"Initializing test setup...\")\n",
    "dataset = create_dataset()\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "\n",
    "# Set to evaluation mode if specified\n",
    "if opt.get('eval', False):\n",
    "    print(\"Setting model to evaluation mode...\")\n",
    "    model.eval()\n",
    "\n",
    "# Testing loop\n",
    "print(\"\\nStarting testing...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_processed = 0\n",
    "test_stats = {\n",
    "    'total_time': 0,\n",
    "    'processing_times': []\n",
    "}\n",
    "\n",
    "for i, data in enumerate(dataset):\n",
    "    if i >= opt.get('num_test', float('inf')):\n",
    "        break\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process image\n",
    "    model.set_input(data)\n",
    "    model.test()\n",
    "    \n",
    "    # Get results\n",
    "    visuals = model.get_current_visuals()\n",
    "    img_path = model.get_image_paths()\n",
    "    \n",
    "    # Record processing time\n",
    "    processing_time = time.time() - start_time\n",
    "    test_stats['processing_times'].append(processing_time)\n",
    "    test_stats['total_time'] += processing_time\n",
    "    \n",
    "    # Display results\n",
    "    display_test_results(visuals, img_path, i)\n",
    "    \n",
    "    total_processed += 1\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"\\nProcessed {i + 1} images...\")\n",
    "        print(f\"Average processing time: {np.mean(test_stats['processing_times']):.3f} seconds\")\n",
    "\n",
    "# Final Statistics\n",
    "print(\"\\n=== Testing Complete ===\")\n",
    "print(f\"Total images processed: {total_processed}\")\n",
    "print(f\"Total time: {test_stats['total_time']:.2f} seconds\")\n",
    "print(f\"Average time per image: {test_stats['total_time']/total_processed:.2f} seconds\")\n",
    "print(f\"Fastest processing: {min(test_stats['processing_times']):.2f} seconds\")\n",
    "print(f\"Slowest processing: {max(test_stats['processing_times']):.2f} seconds\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
